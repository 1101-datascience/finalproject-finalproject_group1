# CM
train_CM <- table(data.frame(truth=train_set$TAIEX..t.,pred=factor(ifelse(predict(model, type="class")>threshold,1,0),levels=0:1)))
test_CM <- table(data.frame(truth=test_set$TAIEX..t.,pred=factor(ifelse(predict(model, newdata=test_set, type="class")>threshold,1,0),levels=0:1)))
vali_CM <- table(data.frame(truth=vali_set$TAIEX..t.,pred=factor(ifelse(predict(model, newdata=vali_set, type="class")>threshold,1,0),levels=0:1)))
training_ACC<-c(training_ACC,sum(diag(train_CM))/sum(train_CM))
test_ACC<-c(test_ACC,sum(diag(test_CM))/sum(test_CM))
validation_ACC <- c(validation_ACC,sum(diag(vali_CM))/sum(vali_CM))
training_recall<-c(training_recall,train_CM[2,2]/(train_CM[2,2]+train_CM[2,1]))
test_recall<-c(test_recall,test_CM[2,2]/(test_CM[2,2]+test_CM[2,1]))
validation_recall<-c(validation_recall,vali_CM[2,2]/(vali_CM[2,2]+vali_CM[2,1]))
training_p<-c(training_p,train_CM[2,2]/(train_CM[2,2]+train_CM[2,1]))
test_p<-c(test_p,test_CM[2,2]/(test_CM[2,2]+test_CM[2,1]))
validation_p<-c(validation_p,vali_CM[2,2]/(vali_CM[2,2]+vali_CM[2,1]))
#nu
nu_train_CM <- table(data.frame(truth=train_set$TAIEX..t.,pred=factor(rep(max_class,nrow(train_set)),levels=0:1)))
nu_test_CM <- table(data.frame(truth=test_set$TAIEX..t.,pred=factor(rep(max_class,nrow(test_set)),levels=0:1)))
nu_vali_CM <- table(data.frame(truth=vali_set$TAIEX..t.,pred=factor(rep(max_class,nrow(vali_set)),levels=0:1)))
# AUC
nu_train_auc<-performance(prediction(rep(max_class,nrow(train_set)),train_set$TAIEX..t.),"auc")@y.values[[1]]
nu_test_auc<-performance(prediction(rep(max_class,nrow(test_set)),test_set$TAIEX..t.),"auc")@y.values[[1]]
nu_vali_auc<-performance(prediction(rep(max_class,nrow(vali_set)),vali_set$TAIEX..t.),"auc")@y.values[[1]]
nu_training_AUC<-c(nu_training_AUC,nu_train_auc)
nu_test_AUC<-c(nu_test_AUC,nu_test_auc)
nu_validation_AUC <- c(nu_validation_AUC,nu_vali_auc)
nu_training_ACC<-c(nu_training_ACC,sum(diag(nu_train_CM))/sum(nu_train_CM))
nu_test_ACC<-c(nu_test_ACC,sum(diag(nu_test_CM))/sum(nu_test_CM))
nu_validation_ACC <- c(nu_validation_ACC,sum(diag(nu_vali_CM))/sum(nu_vali_CM))
nu_training_recall<-c(nu_training_recall,nu_train_CM[2,2]/(nu_train_CM[2,2]+nu_train_CM[2,1]))
nu_test_recall<-c(nu_test_recall,nu_test_CM[2,2]/(nu_test_CM[2,2]+nu_test_CM[2,1]))
nu_validation_recall<-c(nu_validation_recall,nu_vali_CM[2,2]/(nu_vali_CM[2,2]+nu_vali_CM[2,1]))
nu_training_p<-c(nu_training_p,nu_train_CM[2,2]/(nu_train_CM[2,2]+nu_train_CM[2,1]))
nu_test_p<-c(nu_test_p,nu_test_CM[2,2]/(nu_test_CM[2,2]+nu_test_CM[2,1]))
nu_validation_p<-c(nu_validation_p,nu_vali_CM[2,2]/(nu_vali_CM[2,2]+nu_vali_CM[2,1]))
}
round_avg<-function(x){
return(c(round(x,2),round(sum(x)/length(x),2)))
}
set<-c(set,"ave.")
training_AUC<-round_avg(training_AUC)
test_AUC<-round_avg(test_AUC)
validation_AUC<-round_avg(validation_AUC)
nu_training_AUC <- round_avg(nu_training_AUC)
nu_test_AUC<-round_avg(nu_test_AUC)
nu_validation_AUC <- round_avg(nu_validation_AUC)
training_ACC <- round_avg(training_ACC)
validation_ACC <- round_avg(validation_ACC)
test_ACC<-round_avg(test_ACC)
nu_training_ACC <- round_avg(nu_training_ACC)
nu_validation_ACC <- round_avg(nu_validation_ACC)
nu_test_ACC<-round_avg(nu_test_ACC)
training_recall <-round_avg(training_recall)
validation_recall <- round_avg(validation_recall)
test_recall<-round_avg(test_recall)
nu_training_recall <- round_avg(nu_training_recall)
nu_validation_recall <- round_avg(nu_validation_recall)
nu_test_recall<-round_avg(nu_test_recall)
training_p <- round_avg(training_p)
validation_p <- round_avg(validation_p)
test_p<-round_avg(test_p)
nu_training_p <- round_avg(nu_training_p)
nu_validation_p <- round_avg(nu_validation_p)
nu_test_p<-round_avg(nu_test_p)
out_data<-data.frame( AUC=set,
train_AUC =	training_AUC,
validation_AUC =	validation_AUC,
test_AUC=	test_AUC,
ACC=set,
train_ACC =	training_ACC ,
validation_ACC =	validation_ACC ,
test_ACC=	test_ACC,
recall =set,
train_recall =	training_recall ,
test_recall=	test_recall,
validation_recall =	validation_recall ,
percion=set,
training_p =	training_p ,
test_p=	test_p,
validation_p =	validation_p ,
stringsAsFactors = F)
out_data1<-data.frame(AUC=set,
nu_train_AUC =	nu_training_AUC,
nu_validation_AUC =	nu_validation_AUC,
nu_test_AUC=	nu_test_AUC,
ACC=set,
nu_train_ACC =	nu_training_ACC ,
nu_test_ACC=	nu_test_ACC,
nu_validation_ACC =	nu_validation_ACC ,
recall =set,
nu_train_recall =	nu_training_recall ,
nu_test_recall=	nu_test_recall,
nu_validation_recall =	nu_validation_recall ,
percion=set,
nu_training_p =	nu_training_p ,
nu_test_p=	nu_test_p,
nu_validation_p =	nu_validation_p ,
stringsAsFactors = F)
write.table(out_data,file=report_path,quote=FALSE,sep=",",row.names=FALSE, na = "NA",append = TRUE)
write.table(out_data,file=report_path,quote=FALSE,sep=",",row.names=FALSE, na = "NA",append = TRUE)
model <- svm(TAIEX..t.~.,
data=train_data,
probability=TRUE,
scale = TRUE,
kernel='linear',
cost=1,
epsilon=0.1,tolerance=0.01
)
jpeg(file=AUC_jpg_path)
plot(performance(prediction(predict(model),train_data$TAIEX..t.), "tpr","fpr"))
dev.off()
setwd("~/GitHub/finalproject-finalproject_group1")
# group_index<-function(data_rownames,k){
#   set.seed(seed)
#   return (sample(rep(1:k,ceiling(length(data_rownames)/k))[1:length(data_rownames)]))
# }
# combine_index<-function(j){
#   tmp<-c(ans_0[ans0_k_idx==j],ans_1[ans1_k_idx==j])
#   return (tmp[!is.na(tmp)])
# }
#
#
#
#package
requrie_packages <- c("mice","e1071","ROCR")
new_packages <- requrie_packages[!(requrie_packages %in% row.names(installed.packages()))]
if(length(new_packages)>0){
print("install require packages")
install.packages(new_packages)
print("finish install require packages")}
library(ROCR)
library(e1071)
library(caret)
#param
# args = commandArgs(trailingOnly=TRUE)
# k <- as.numeric(args[2])
train_path <- args[2]
report_path <- args[4]
AUC_jpg_path<- args[6]
# predict_path<-args[10]
#
seed<-10
set.seed(seed)
#
train_path<-"data/ourdata.csv"
# test_path <-"Data/test.csv"
report_path <-"ploy_cmd_report_svm.csv"
AUC_jpg_path<-"SVM_AUC.jpg"
# predict_path <-"cmd_predict_svm.csv"
#
k<-5
train_data <- read.csv(train_path)
str(train_data)
max_class<-1
group_index<-sample(rep(1:k,nrow(train_data)/5+1,replace=F)[1:nrow(train_data)])
set <- c()
training_AUC <- c()
validation_AUC <- c()
test_AUC<-c()
nu_training_AUC <- c()
nu_validation_AUC <- c()
nu_test_AUC<-c()
training_ACC <- c()
validation_ACC <- c()
test_ACC<-c()
nu_training_ACC <- c()
nu_validation_ACC <- c()
nu_test_ACC<-c()
training_recall <- c()
validation_recall <- c()
test_recall<-c()
nu_training_recall <- c()
nu_validation_recall <- c()
nu_test_recall<-c()
training_p <- c()
validation_p <- c()
test_p<-c()
nu_training_p <- c()
nu_validation_p <- c()
nu_test_p<-c()
threshold<-0.5
for (i in 1:k){
print(paste(paste("=======iteration:",as.character(i)),"=======")) #直接取出平軍分配後的rownames
test_set<- train_data[which(group_index==i),]
vali_set<- train_data[which(group_index==(i%%k)+1),]
train_set<-train_data[-(c(which(group_index==i),which(group_index==(i%%k)+1))),]
model <- svm(TAIEX..t.~.,
data=train_set,
probability=TRUE,
scale = TRUE,
kernel='linear',
cost=1,
epsilon=0.1,tolerance=0.01
)
# AUC
train_auc<-performance(prediction(predict(model, probability=TRUE),train_set$TAIEX..t.),"auc")@y.values[[1]]
test_pred<-predict(model, newdata=test_set, probability=TRUE)
test_auc<-performance(prediction(test_pred,test_set$TAIEX..t.),"auc")@y.values[[1]]
vali_pred<-predict(model, newdata=vali_set, probability=TRUE )
vali_auc<-performance(prediction(vali_pred,vali_set$TAIEX..t.),"auc")@y.values[[1]]
set<-c(set,paste0("fold",as.character(i)))
training_AUC<-c(training_AUC,train_auc)
test_AUC<-c(test_AUC,test_auc)
validation_AUC <- c(validation_AUC,vali_auc)
# CM
train_CM <- table(data.frame(truth=train_set$TAIEX..t.,pred=factor(ifelse(predict(model, type="class")>threshold,1,0),levels=0:1)))
test_CM <- table(data.frame(truth=test_set$TAIEX..t.,pred=factor(ifelse(predict(model, newdata=test_set, type="class")>threshold,1,0),levels=0:1)))
vali_CM <- table(data.frame(truth=vali_set$TAIEX..t.,pred=factor(ifelse(predict(model, newdata=vali_set, type="class")>threshold,1,0),levels=0:1)))
training_ACC<-c(training_ACC,sum(diag(train_CM))/sum(train_CM))
test_ACC<-c(test_ACC,sum(diag(test_CM))/sum(test_CM))
validation_ACC <- c(validation_ACC,sum(diag(vali_CM))/sum(vali_CM))
training_recall<-c(training_recall,train_CM[2,2]/(train_CM[2,2]+train_CM[2,1]))
test_recall<-c(test_recall,test_CM[2,2]/(test_CM[2,2]+test_CM[2,1]))
validation_recall<-c(validation_recall,vali_CM[2,2]/(vali_CM[2,2]+vali_CM[2,1]))
training_p<-c(training_p,train_CM[2,2]/(train_CM[2,2]+train_CM[2,1]))
test_p<-c(test_p,test_CM[2,2]/(test_CM[2,2]+test_CM[2,1]))
validation_p<-c(validation_p,vali_CM[2,2]/(vali_CM[2,2]+vali_CM[2,1]))
#nu
nu_train_CM <- table(data.frame(truth=train_set$TAIEX..t.,pred=factor(rep(max_class,nrow(train_set)),levels=0:1)))
nu_test_CM <- table(data.frame(truth=test_set$TAIEX..t.,pred=factor(rep(max_class,nrow(test_set)),levels=0:1)))
nu_vali_CM <- table(data.frame(truth=vali_set$TAIEX..t.,pred=factor(rep(max_class,nrow(vali_set)),levels=0:1)))
# AUC
nu_train_auc<-performance(prediction(rep(max_class,nrow(train_set)),train_set$TAIEX..t.),"auc")@y.values[[1]]
nu_test_auc<-performance(prediction(rep(max_class,nrow(test_set)),test_set$TAIEX..t.),"auc")@y.values[[1]]
nu_vali_auc<-performance(prediction(rep(max_class,nrow(vali_set)),vali_set$TAIEX..t.),"auc")@y.values[[1]]
nu_training_AUC<-c(nu_training_AUC,nu_train_auc)
nu_test_AUC<-c(nu_test_AUC,nu_test_auc)
nu_validation_AUC <- c(nu_validation_AUC,nu_vali_auc)
nu_training_ACC<-c(nu_training_ACC,sum(diag(nu_train_CM))/sum(nu_train_CM))
nu_test_ACC<-c(nu_test_ACC,sum(diag(nu_test_CM))/sum(nu_test_CM))
nu_validation_ACC <- c(nu_validation_ACC,sum(diag(nu_vali_CM))/sum(nu_vali_CM))
nu_training_recall<-c(nu_training_recall,nu_train_CM[2,2]/(nu_train_CM[2,2]+nu_train_CM[2,1]))
nu_test_recall<-c(nu_test_recall,nu_test_CM[2,2]/(nu_test_CM[2,2]+nu_test_CM[2,1]))
nu_validation_recall<-c(nu_validation_recall,nu_vali_CM[2,2]/(nu_vali_CM[2,2]+nu_vali_CM[2,1]))
nu_training_p<-c(nu_training_p,nu_train_CM[2,2]/(nu_train_CM[2,2]+nu_train_CM[2,1]))
nu_test_p<-c(nu_test_p,nu_test_CM[2,2]/(nu_test_CM[2,2]+nu_test_CM[2,1]))
nu_validation_p<-c(nu_validation_p,nu_vali_CM[2,2]/(nu_vali_CM[2,2]+nu_vali_CM[2,1]))
}
round_avg<-function(x){
return(c(round(x,2),round(sum(x)/length(x),2)))
}
set<-c(set,"ave.")
training_AUC<-round_avg(training_AUC)
test_AUC<-round_avg(test_AUC)
validation_AUC<-round_avg(validation_AUC)
nu_training_AUC <- round_avg(nu_training_AUC)
nu_test_AUC<-round_avg(nu_test_AUC)
nu_validation_AUC <- round_avg(nu_validation_AUC)
training_ACC <- round_avg(training_ACC)
validation_ACC <- round_avg(validation_ACC)
test_ACC<-round_avg(test_ACC)
nu_training_ACC <- round_avg(nu_training_ACC)
nu_validation_ACC <- round_avg(nu_validation_ACC)
nu_test_ACC<-round_avg(nu_test_ACC)
training_recall <-round_avg(training_recall)
validation_recall <- round_avg(validation_recall)
test_recall<-round_avg(test_recall)
nu_training_recall <- round_avg(nu_training_recall)
nu_validation_recall <- round_avg(nu_validation_recall)
nu_test_recall<-round_avg(nu_test_recall)
training_p <- round_avg(training_p)
validation_p <- round_avg(validation_p)
test_p<-round_avg(test_p)
nu_training_p <- round_avg(nu_training_p)
nu_validation_p <- round_avg(nu_validation_p)
nu_test_p<-round_avg(nu_test_p)
out_data<-data.frame( AUC=set,
train_AUC =	training_AUC,
validation_AUC =	validation_AUC,
test_AUC=	test_AUC,
ACC=set,
train_ACC =	training_ACC ,
validation_ACC =	validation_ACC ,
test_ACC=	test_ACC,
recall =set,
train_recall =	training_recall ,
test_recall=	test_recall,
validation_recall =	validation_recall ,
percion=set,
training_p =	training_p ,
test_p=	test_p,
validation_p =	validation_p ,
stringsAsFactors = F)
out_data1<-data.frame(AUC=set,
nu_train_AUC =	nu_training_AUC,
nu_validation_AUC =	nu_validation_AUC,
nu_test_AUC=	nu_test_AUC,
ACC=set,
nu_train_ACC =	nu_training_ACC ,
nu_test_ACC=	nu_test_ACC,
nu_validation_ACC =	nu_validation_ACC ,
recall =set,
nu_train_recall =	nu_training_recall ,
nu_test_recall=	nu_test_recall,
nu_validation_recall =	nu_validation_recall ,
percion=set,
nu_training_p =	nu_training_p ,
nu_test_p=	nu_test_p,
nu_validation_p =	nu_validation_p ,
stringsAsFactors = F)
write.table(out_data,file=report_path,quote=FALSE,sep=",",row.names=FALSE, na = "NA",append = TRUE)
write.table(out_data,file=report_path,quote=FALSE,sep=",",row.names=FALSE, na = "NA",append = TRUE)
model <- svm(TAIEX..t.~.,
data=train_data,
probability=TRUE,
scale = TRUE,
kernel='linear',
cost=1,
epsilon=0.1,tolerance=0.01
)
jpeg(file=AUC_jpg_path)
plot(performance(prediction(predict(model),train_data$TAIEX..t.), "tpr","fpr"))
dev.off()
# group_index<-function(data_rownames,k){
#   set.seed(seed)
#   return (sample(rep(1:k,ceiling(length(data_rownames)/k))[1:length(data_rownames)]))
# }
# combine_index<-function(j){
#   tmp<-c(ans_0[ans0_k_idx==j],ans_1[ans1_k_idx==j])
#   return (tmp[!is.na(tmp)])
# }
#
#
#
#package
requrie_packages <- c("mice","e1071","ROCR")
new_packages <- requrie_packages[!(requrie_packages %in% row.names(installed.packages()))]
if(length(new_packages)>0){
print("install require packages")
install.packages(new_packages)
print("finish install require packages")}
library(ROCR)
library(e1071)
library(caret)
#param
# args = commandArgs(trailingOnly=TRUE)
# k <- as.numeric(args[2])
train_path <- args[2]
report_path <- args[4]
AUC_jpg_path<- args[6]
# predict_path<-args[10]
#
seed<-10
set.seed(seed)
#
train_path<-"data/ourdata.csv"
# test_path <-"Data/test.csv"
report_path <-"ploy_cmd_report_svm.csv"
AUC_jpg_path<-"SVM_AUC.jpg"
# predict_path <-"cmd_predict_svm.csv"
#
k<-5
train_data <- read.csv(train_path)
str(train_data)
max_class<-1
group_index<-sample(rep(1:k,nrow(train_data)/5+1,replace=F)[1:nrow(train_data)])
set <- c()
training_AUC <- c()
validation_AUC <- c()
test_AUC<-c()
nu_training_AUC <- c()
nu_validation_AUC <- c()
nu_test_AUC<-c()
training_ACC <- c()
validation_ACC <- c()
test_ACC<-c()
nu_training_ACC <- c()
nu_validation_ACC <- c()
nu_test_ACC<-c()
training_recall <- c()
validation_recall <- c()
test_recall<-c()
nu_training_recall <- c()
nu_validation_recall <- c()
nu_test_recall<-c()
training_p <- c()
validation_p <- c()
test_p<-c()
nu_training_p <- c()
nu_validation_p <- c()
nu_test_p<-c()
threshold<-0.5
for (i in 1:k){
print(paste(paste("=======iteration:",as.character(i)),"=======")) #直接取出平軍分配後的rownames
test_set<- train_data[which(group_index==i),]
vali_set<- train_data[which(group_index==(i%%k)+1),]
train_set<-train_data[-(c(which(group_index==i),which(group_index==(i%%k)+1))),]
model <- svm(TAIEX..t.~.,
data=train_set,
probability=TRUE,
scale = TRUE,
kernel='linear',
cost=1,
epsilon=0.1,tolerance=0.01
)
# AUC
train_auc<-performance(prediction(predict(model, probability=TRUE),train_set$TAIEX..t.),"auc")@y.values[[1]]
test_pred<-predict(model, newdata=test_set, probability=TRUE)
test_auc<-performance(prediction(test_pred,test_set$TAIEX..t.),"auc")@y.values[[1]]
vali_pred<-predict(model, newdata=vali_set, probability=TRUE )
vali_auc<-performance(prediction(vali_pred,vali_set$TAIEX..t.),"auc")@y.values[[1]]
set<-c(set,paste0("fold",as.character(i)))
training_AUC<-c(training_AUC,train_auc)
test_AUC<-c(test_AUC,test_auc)
validation_AUC <- c(validation_AUC,vali_auc)
# CM
train_CM <- table(data.frame(truth=train_set$TAIEX..t.,pred=factor(ifelse(predict(model, type="class")>threshold,1,0),levels=0:1)))
test_CM <- table(data.frame(truth=test_set$TAIEX..t.,pred=factor(ifelse(predict(model, newdata=test_set, type="class")>threshold,1,0),levels=0:1)))
vali_CM <- table(data.frame(truth=vali_set$TAIEX..t.,pred=factor(ifelse(predict(model, newdata=vali_set, type="class")>threshold,1,0),levels=0:1)))
training_ACC<-c(training_ACC,sum(diag(train_CM))/sum(train_CM))
test_ACC<-c(test_ACC,sum(diag(test_CM))/sum(test_CM))
validation_ACC <- c(validation_ACC,sum(diag(vali_CM))/sum(vali_CM))
training_recall<-c(training_recall,train_CM[2,2]/(train_CM[2,2]+train_CM[2,1]))
test_recall<-c(test_recall,test_CM[2,2]/(test_CM[2,2]+test_CM[2,1]))
validation_recall<-c(validation_recall,vali_CM[2,2]/(vali_CM[2,2]+vali_CM[2,1]))
training_p<-c(training_p,train_CM[2,2]/(train_CM[2,2]+train_CM[2,1]))
test_p<-c(test_p,test_CM[2,2]/(test_CM[2,2]+test_CM[2,1]))
validation_p<-c(validation_p,vali_CM[2,2]/(vali_CM[2,2]+vali_CM[2,1]))
#nu
nu_train_CM <- table(data.frame(truth=train_set$TAIEX..t.,pred=factor(rep(max_class,nrow(train_set)),levels=0:1)))
nu_test_CM <- table(data.frame(truth=test_set$TAIEX..t.,pred=factor(rep(max_class,nrow(test_set)),levels=0:1)))
nu_vali_CM <- table(data.frame(truth=vali_set$TAIEX..t.,pred=factor(rep(max_class,nrow(vali_set)),levels=0:1)))
# AUC
nu_train_auc<-performance(prediction(rep(max_class,nrow(train_set)),train_set$TAIEX..t.),"auc")@y.values[[1]]
nu_test_auc<-performance(prediction(rep(max_class,nrow(test_set)),test_set$TAIEX..t.),"auc")@y.values[[1]]
nu_vali_auc<-performance(prediction(rep(max_class,nrow(vali_set)),vali_set$TAIEX..t.),"auc")@y.values[[1]]
nu_training_AUC<-c(nu_training_AUC,nu_train_auc)
nu_test_AUC<-c(nu_test_AUC,nu_test_auc)
nu_validation_AUC <- c(nu_validation_AUC,nu_vali_auc)
nu_training_ACC<-c(nu_training_ACC,sum(diag(nu_train_CM))/sum(nu_train_CM))
nu_test_ACC<-c(nu_test_ACC,sum(diag(nu_test_CM))/sum(nu_test_CM))
nu_validation_ACC <- c(nu_validation_ACC,sum(diag(nu_vali_CM))/sum(nu_vali_CM))
nu_training_recall<-c(nu_training_recall,nu_train_CM[2,2]/(nu_train_CM[2,2]+nu_train_CM[2,1]))
nu_test_recall<-c(nu_test_recall,nu_test_CM[2,2]/(nu_test_CM[2,2]+nu_test_CM[2,1]))
nu_validation_recall<-c(nu_validation_recall,nu_vali_CM[2,2]/(nu_vali_CM[2,2]+nu_vali_CM[2,1]))
nu_training_p<-c(nu_training_p,nu_train_CM[2,2]/(nu_train_CM[2,2]+nu_train_CM[2,1]))
nu_test_p<-c(nu_test_p,nu_test_CM[2,2]/(nu_test_CM[2,2]+nu_test_CM[2,1]))
nu_validation_p<-c(nu_validation_p,nu_vali_CM[2,2]/(nu_vali_CM[2,2]+nu_vali_CM[2,1]))
}
round_avg<-function(x){
return(c(round(x,2),round(sum(x)/length(x),2)))
}
set<-c(set,"ave.")
training_AUC<-round_avg(training_AUC)
test_AUC<-round_avg(test_AUC)
validation_AUC<-round_avg(validation_AUC)
nu_training_AUC <- round_avg(nu_training_AUC)
nu_test_AUC<-round_avg(nu_test_AUC)
nu_validation_AUC <- round_avg(nu_validation_AUC)
training_ACC <- round_avg(training_ACC)
validation_ACC <- round_avg(validation_ACC)
test_ACC<-round_avg(test_ACC)
nu_training_ACC <- round_avg(nu_training_ACC)
nu_validation_ACC <- round_avg(nu_validation_ACC)
nu_test_ACC<-round_avg(nu_test_ACC)
training_recall <-round_avg(training_recall)
validation_recall <- round_avg(validation_recall)
test_recall<-round_avg(test_recall)
nu_training_recall <- round_avg(nu_training_recall)
nu_validation_recall <- round_avg(nu_validation_recall)
nu_test_recall<-round_avg(nu_test_recall)
training_p <- round_avg(training_p)
validation_p <- round_avg(validation_p)
test_p<-round_avg(test_p)
nu_training_p <- round_avg(nu_training_p)
nu_validation_p <- round_avg(nu_validation_p)
nu_test_p<-round_avg(nu_test_p)
out_data<-data.frame( AUC=set,
train_AUC =	training_AUC,
validation_AUC =	validation_AUC,
test_AUC=	test_AUC,
ACC=set,
train_ACC =	training_ACC ,
validation_ACC =	validation_ACC ,
test_ACC=	test_ACC,
recall =set,
train_recall =	training_recall ,
test_recall=	test_recall,
validation_recall =	validation_recall ,
percion=set,
training_p =	training_p ,
test_p=	test_p,
validation_p =	validation_p ,
stringsAsFactors = F)
out_data1<-data.frame(AUC=set,
nu_train_AUC =	nu_training_AUC,
nu_validation_AUC =	nu_validation_AUC,
nu_test_AUC=	nu_test_AUC,
ACC=set,
nu_train_ACC =	nu_training_ACC ,
nu_test_ACC=	nu_test_ACC,
nu_validation_ACC =	nu_validation_ACC ,
recall =set,
nu_train_recall =	nu_training_recall ,
nu_test_recall=	nu_test_recall,
nu_validation_recall =	nu_validation_recall ,
percion=set,
nu_training_p =	nu_training_p ,
nu_test_p=	nu_test_p,
nu_validation_p =	nu_validation_p ,
stringsAsFactors = F)
write.table(out_data,file=report_path,quote=FALSE,sep=",",row.names=FALSE, na = "NA",append = TRUE)
write.table(out_data1,file=report_path,quote=FALSE,sep=",",row.names=FALSE, na = "NA",append = TRUE)
model <- svm(TAIEX..t.~.,
data=train_data,
probability=TRUE,
scale = TRUE,
kernel='linear',
cost=1,
epsilon=0.1,tolerance=0.01
)
jpeg(file=AUC_jpg_path)
plot(performance(prediction(predict(model),train_data$TAIEX..t.), "tpr","fpr"))
dev.off()
